#
# Work based on https://github.com/mmistakes/minimal-mistakes/blob/master/_data/ui-text.yml
#
# Configuration: Add to _config.yml
# locale: "en-US"  [YOUR PREFERRED LOCALE]
#
# How to use:
# {{ site.data.ui-text[site.locale].<var_name> | default: 'text' }}

# English (default)
# -----------------
en: &DEFAULT_EN
  header:
    title: A Data Story on
    text: Natural Disasters in Quotebank
    button: Let's Explore!
    buttonlink: "#abstract"

  about:
    title: "Abstract"
    text: "A brief introduction to the project"
    body: "Every year, natural disasters happen and often take many lives. After such events, the 
    pages of newspapers are full of quotes from people expressing regret for the unfortunate event. 
    These events often remain in people's memory for a lifetime. What influences how long these 
    events will be talked about? In this research project, we explore how much is said about the 
    biggest earthquakes after they have occurred and what factors influence this. We will look for answers in the 
    [Quotebank](https://github.com/epfl-dlab/Quotebank) 2008-2020 quotes on disasters taken from the
     [international disasters database](https://public.emdat.be/data) combined with world 
     development indicators from the [World Data Bank](https://databank.worldbank.org/source/world-development-indicators). 
     To simplify disaster quote detection, we will further look into classifying the quotes by 
     whether they talk about a disaster or not."
    section: abstract

  research:
    title: "Research Questions"
    text: "Questions tackled in the research"
    body: "In this research project, we explore two questions.
    <br><br>
    First, how correctly will NLP models trained on disaster tweets like in 
    [this Kaggle challenge](https://www.kaggle.com/c/nlp-getting-started/overview) generalize to 
    classifying disaster quotes in Quotebank? This question is important in respect of robustness 
    analysis of models and their transfer learning capabilities.
    <br><br>
    Second, what factors influence how long an earthquake will be talked about in Quotebank quotes 
    from 2008 to 2020? The interesting factors include total deaths, total damage in dollars, 
    country of disaster, wealth indicators of the country, etc.
    <br><br>
    There are various other research questions related to this that are interesting and worth 
    further research, like \"what is the sentiment towards different disasters and why\" and \"how 
    does the country of the speaker affect which disasters he is talking about\"."
    # Given that a comprehensive analysis of these research questions was challenging, we
    # discard other related and interesting questions like
    section: research

  timeline:
    title: "Timeline"
    text: "An introduction of the steps taken on a timeline"
    section: intro
    # left is the default
    #start_align: "left"
    body1: "First, how accurate is the NLP model trained in disaster tweets generalized to the 
    classification of disaster estimates in Quotebank? Second, what are the factors that influence 
    the duration of the earthquake in the Quotebank quote from 2008 to 2020?
    <br><br>
    To answer these questions, we will use a timeline to visualize the steps taken in this section."
    events:
      - title: "Always Need More Data!"
        #        year: "March 2011"
        desc: "We started with gathering data for our project, preprocessing and visualizing it. We 
        gathered data from indicators of the severity of a disaster, from the development of 
        different countries in different years, from domain geographics and of course, Twitter."
        image: assets/img/intro/tape.png
        alt: Tape
      - title: "NLP Models"
        #        year: "2009-2011"
        desc: "We used a combination of SotA NLP models that are trained on the 
        [Twitter Disaster Dataset](https://www.kaggle.com/c/nlp-getting-started/data) to detect the 
        disasters in the quotes."
        image: assets/img/intro/twitter.png
        alt: Twitter logo
      - title: "Filtering Quotes with RegEx"
#        year: "March 2011"
        desc: "We also filtered earthquakes from QuoteBank using keyword regex. For the disasters 
        we gathered the most important keywords, such as the associated event name."
        image: assets/img/intro/filter.png
        alt: Filter
      - title: "Media Coverage Analysis"
        desc: "We analyzed in depth the possible factors of how much people talk about these 
        horrible events."
        image: assets/img/intro/media.png
        alt: Media
    end: "How about <br> some more <br> details?"
  approach:
    title: "Approach"
    text: "A brief introduction to the project"
    body: "Here we can give a brief introduction to our methodologies and the data we used to create the project."
    section: approach
    additional-datasets:
      title: "Additional Datasets"
      text: "Additional datasets used during the project"
      datasets:
        - title: "The International Disasters Database"
          link: https://public.emdat.be/   # can be left empty
          section: int-disasters
          desc: "We use [the international disasters database](https://public.emdat.be/data) to 
          introduce natural disasters of this century with their most important attributes. This 
          dataset was compiled from various sources including UN, governmental and non-governmental 
          agencies, insurance companies, research institutes, and press agencies. In the majority of 
          cases, a disaster will only be entered into EM-DAT if at least two sources report the 
          disaster's occurrence in terms of deaths and/or affected persons.
          <br><br>
          Below, we present the number of disasters per country on the world map."
          image:
            src: assets/img/approach/num_disasters_per_country.png
            alt: Number of disasters per country
        - title: "World Development Indicators"
          link: https://databank.worldbank.org/source/world-development-indicators   # can be left empty
          section: wdi
          desc: "One important factor in how much people talk about a disaster might be the country 
          and its attributes. In this [dataset](https://databank.worldbank.org/source/world-development-indicators), 
          the most important development indicators of the 
          country can be found, for example GDP, population, and fertility rate. Detailed 
          per-indicator source and description is given in `databank_wdi_metadata.csv`. We would 
          like to observe whether there is a connection between these indicators and the length 
          and distribution of time they talk about the disaster. 
          <br><br>
          Below, we present the correlation of the development indicators as a heatmap."
          image:
            src: assets/img/approach/wdi.png
            alt: Correlation of World Development Indicators
        - title: "GDELT Geographic Lookup of Domains"
          link: https://blog.gdeltproject.org/mapping-the-media-a-geographic-lookup-of-gdelts-sources/   # can be left empty
          section: gdelt
          desc: "The geographical location of newspapers could affect the citations contained in 
          them. Although the quotes in the Quotebank dataset contain links to the article in which 
          they were found, we cannot find out the true geographical location of the news source 
          from the link itself. E.g. theguardian.com and nytimes.com both use .com top-level domain, 
          but they are reporting events in different countries. That's why we decided to choose a 
          [GDELT dataset](https://blog.gdeltproject.org/mapping-the-media-a-geographic-lookup-of-gdelts-sources/) 
          that associates a particular domain with the right country from which that 
          news source comes. This dataset was created from the enormous GDELT dataset and used the 
          fact that news outlets cover events physically proximate to them far more often than they 
          do events on the other side of the world."
        - title: "Twitter Disaster Dataset"
          link: https://www.kaggle.com/c/nlp-getting-started/data   # can be left empty
          section: twitter-disaster-dataset
          desc: "The Twitter Disaster Dataset is a collection of tweets from Twitter users about 
          natural disasters. The dataset is available for download from the 
          [Kaggle](https://www.kaggle.com/c/nlp-getting-started) website.
          <br><br>
          In this dataset we have a collection of tweets about disasters and random topics from 
          Twitter users. The tweets are labeled whether they are about a disaster or not and we 
          use them to train NLP models for transfer learning. The structure of the dataset is as 
          follows:
          <br><br>
          Columns<br>
          &emsp;`id` - a unique identifier for each tweet<br>
          &emsp;`text` - the text of the tweet<br>
          &emsp;`location` - the location the tweet was sent from (may be blank)<br>
          &emsp;`keyword` - a particular keyword from the tweet (may be blank)<br>
          &emsp;`target` - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)<br>"
    nlp-tweets:
      title: "NLP from Tweets"
      text: "Training transformers on Tweets and evaluating on the Quotebank"
      body1: "To simplify the classification of quotes by whether they talk about disasters or not, 
      we looked into transferring a model trained to classify whether a Tweet is talking about a 
      disaster and using it to classify Quotebank quotes. To do so, we have started with 
      [this Kaggle challenge](https://www.kaggle.com/c/nlp-getting-started/overview). Twitter has 
      become an important communication channel in times of emergency, which is why the Kaggle 
      challenge authors' argument for more agencies being interested in programmatically monitoring 
      Twitter for disaster detection (i.e. disaster relief organizations and news agencies). In 
      that competition, data scientists are challenged to build a machine learning model that 
      predicts which Tweets are about real disasters and which one’s aren’t. The challenge 
      participants are provided with a dataset of 10,000 tweets that were hand classified, with a 
      disclaimer of the dataset containing text that may be considered profane, vulgar, or 
      offensive. To give a taste of how [these tweets](https://github.com/epfl-ada/ada-2021-project-hivemind/blob/main/datasets/disaster-tweets/test.csv) 
      look like, consider the following examples:"
      body2: "These small texts are labeled as talking about a disaster in a general sense of 
      disasters (accident, attack, battle, catastrophe, etc.) that we will refer to as general-disaster. 
      We are, however, interested in natural disasters like earthquakes, but a successfully 
      performing general-disaster classification model could still be used to narrow down the 
      search for natural disasters in a dataset as big as Quotebank."
      image1:
        src: assets/img/approach/bert_distilbert.jpg
        alt: "Bert and DistilBert"
      body3: "Among the numerous models submitted for the challenge, we have picked two with high 
      test scores, one of which is a `BERT` transformer and the other a `DISTILBERT`. We will use 
      `BERT` and `DISTILBERT` as their code names. We have modified them, performed a small 
      hyperparameter search to find the best parameters, and trained the final model on all 
      available data (`train`+`test`). Their respective test performance on the tweets dataset is 
      shown below:"
      image2:
        src: assets/img/approach/cf_bert_distilbert.png
        alt: "Bert and DistilBert Confusion Matrix"
      body4: "Given the big number of quotes in Quotebank (`234'994'989`), the computational time needed to 
      predict the class for all Quotebank quotes was about 80 GPU days. Because of this, we applied 
      the final models on a randomly chosen sample consisting of 0.1% of the whole dataset, which 
      is still an enormous sample of 234'991 quotes and gives tight bounds for the resulting 
      estimates
      <br><br>
      BERT has predicted 9.74% quotes to be disasters, and DISTILBERT slightly less - 8.89%. This, 
      however, seems to be rather unrealistic given the nature of Quotebank quotes - does really 
      about 1 out of 10 quotes talk about a general disaster? To evaluate the precision of these 
      results, we have hand-labeled a subset of 400 positive classification results as true 
      positives or false positives. In the hand labeling, we distinguished general disasters the 
      models were trained on from natural disasters we are interested in. The resulting precisions 
      are, however, quite low and are shown in the table below:"
      body5: "DISTILBERT had a far better generalization performance, as evaluated by precision, 
      for both general disasters and natural disasters. The transfer learning didn't succeed 
      overall and the models were not robust to the change of text distribution from tweets to 
      Quotebank quotes. This could have been expected given the significant differences in the 
      texts that tweets have compared to quotes: tweets are much shorter, use loads of hashtags, 
      URLs, and emojis, etc."
      body6: "But going beneath just the numbers, we experienced many strange things when labeling 
      the quotes. Firstly, BERT worked almost the opposite way from what we wanted it to, as if it 
      deliberately decided to choose those quotes which have no connection to any disasters. They 
      were about Formula-1, politics, and celebrity news. However, DISTILBERT was more promising. 
      Even though it had many false positives, they would often be quotes about sports, but in the 
      context of sports matches being wars, and people fighting for triumph. For example: “Our 
      defense did a great job. There were some times where they could have been better, but I saw 
      the puck well.” Also, it can be interesting that a lot of quotes were about veterans and their 
      rehabilitation. Fires, health problems, cybercrime, and financial crises were also a frequent 
      part of the false positives. The overall sentiment was also negative in these predictions, 
      which is also something that is expected when predicting the topic of catastrophic events. 
      All in all, none of the models performed well, but DISTILBERT had shown some robustness 
      qualitatively.
      <br><br>
      Below are snippets of positives for BERT and DISTILBERT."
      body7: "For the details of the transfer learning effort, you can head over to the relevant 
      notebooks: [tweets-bert](https://github.com/epfl-ada/ada-2021-project-hivemind/blob/main/tweets/tweets-bert.ipynb), 
      [tweets-transfer-bert](https://github.com/epfl-ada/ada-2021-project-hivemind/blob/main/tweets/tweets-transfer-bert.ipynb), 
      [tweets-transfer-distilbert](https://github.com/epfl-ada/ada-2021-project-hivemind/blob/main/tweets/tweets-transfer-distilbert.ipynb), 
      [quotebank-sample](https://github.com/epfl-ada/ada-2021-project-hivemind/blob/main/tweets/quotebank-sample.ipynb)."
    media-coverage:
      title: "Media Coverage Analysis"
      text: "Analysis of the media coverage of natural disasters in the Quotebank dataset"
      body1: "What makes an earthquake worth to talk about? 
      <br><br>
      All of the news portals are trying to find those events which reach the most readers. Natural 
      disasters are a topic that interests many readers. Naturally, people don’t want to lag behind 
      the world’s most important events, but that people are much more likely to read negative news 
      than positive news, also contributes to that news portals are happy to bring down articles 
      about these events. These tendencies are also true for celebrities, politicians and other 
      media influencers whose thoughts and ideas are quoted in the media.
      <br><br>
      As for other news as well, the number of quotes in total and in the first period after the 
      event is a meaningful indicator of its popularity in media. For the earthquakes we used this 
      measurement to analyse the appearance of the disaster in the news. We also included the 
      length between the first and last appearance of the earthquake in the quotes in the analysis, 
      which happened to be less meaningful in this respect."
      body2: "As we would expect, people tend to talk about more in the first few months after the 
      earthquake about the disaster. As for any news an event is interesting if it happened in the 
      recent past. For the earthquakes, one would expect that its appearance in the news is longer 
      that casual news, because of the post-earthquakes and the long time of recovery of an area. 
      In our analysis, we could also observe a peak of the daily quotes after one year of the date 
      of the event, this can be the result of the anniversary commemorations."
      body3: "However, our main focus was that what can influence the media coverage of the 
      earthquakes. To analyse this question we gathered a lot of data about the disaster itself and 
      its location and examined the correlation between these indicators and the media coverage 
      indicators. What we would expect that those factors which measures the human exposure.
      
      <br><br>
      As it can be seen in the heatmap below, we observed a massive correlation between the number of quotes and
      
      - the number of death associated with the event
      
      - the number of injured
      
      - the reconstruction cost
      
      
      As for the number of quotes in the first year the list expanded by 
      
      - aid contribution
      
      - mortality rate between under 5 years olds
      
      - number of days required to start a business"
      image1:
        src: assets/img/approach/heatmap.png
        alt: Heatmap

  portfolio:
    title: "Visualizations"
    text: "Some visualizations of the data"
    section: portfolio
    closebutton: "Close Project"

  conclusion:
    title: "Conclusion"
    text: "Some closing words on our project"
    body: "All in all, we can see that NLP is an area of ​​science where there is still room for improvement. The simplest regex search method has, in the end, worked the best for detecting the quotes in our use case. Furthermore, from the data set we extracted, we can learn a wide range of valuable information about the world, including what factors make a disaster breakthrough news. As a result, this field is something that has a huge potential to reform science."
    section: conclusion

  team:
    title: "Team"
    text: "The Amazing ADA HiveMind Team"
#    subtext: Lorem ipsum dolor sit amet, consectetur adipisicing elit. Aut eaque, laboriosam veritatis, quos non quis ad perspiciatis, totam corporis ea, alias ut unde. **Markdown** supported.
    section: team
    people:
      - name: "Hilda Horvath"
        image: assets/img/team/hilda.jfif
        social:
          - url: https://github.com/hhildaa
            icon: fab fa-github
        mail: hilda.horvath@epfl.ch
        mailicon: fa fa-envelope
      - name: "Lovro Nuic"
        image: assets/img/team/lovro.jfif
        social:
          - url: https://github.com/lnuic
            icon: fab fa-github
        mail: lovro.nuic@epfl.ch
        mailicon: fa fa-envelope
      - name: "Frano Rajic"
        image: assets/img/team/frano.jfif
        social:
          - url: https://github.com/m43
            icon: fab fa-github
        mail: frano.rajic@epfl.ch
        mailicon: fa fa-envelope
      - name: "Batuhan Faik Derinbay"
        image: assets/img/team/batuhan.jpg
        social:
          - url: https://github.com/batuhanfaik
            icon: fab fa-github
        mail: batuhan.derinbay@epfl.ch
        mailicon: fa fa-envelope
  footer:
    legal: "Privacy Policy"
    social:
      - url: https://github.com/epfl-ada/ada-2021-project-hivemind
        icon: "fab fa-github"
en-US:
  <<: *DEFAULT_EN
en-CA:
  <<: *DEFAULT_EN
en-GB:
  <<: *DEFAULT_EN
en-AU:
  <<: *DEFAULT_EN